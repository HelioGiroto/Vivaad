<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.8/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.8/ http://www.mediawiki.org/xml/export-0.8.xsd" version="0.8" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <base>http://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.21wmf3</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Prisoner's dilemma</title>
    <ns>0</ns>
    <id>43717</id>
    <revision>
      <id>522112851</id>
      <parentid>521149628</parentid>
      <timestamp>2012-11-09T03:07:20Z</timestamp>
      <contributor>
        <username>JYBot</username>
        <id>16596343</id>
      </contributor>
      <minor/>
      <comment>r2.7.1) (Robot: Adding [[is:Vandamál fangans]]</comment>
      <text xml:space="preserve" bytes="43832">{{About|game theory|the 1988 novel|Prisoner's Dilemma (novel)|the Doctor Who audiobook|The Prisoner's Dilemma|the 2001 play|The Prisoner's Dilemma (play)}}
{{citation style|date=October 2012}}
The '''prisoner's dilemma''' is a canonical example of a game analyzed in [[game theory]] that shows why two individuals might not cooperate, even if it appears that it is in their best interests to do so. It was originally framed by [[Merrill Flood]] and [[Melvin Dresher]] working at [[RAND Corporation|RAND]] in 1950. [[Albert W. Tucker]] formalized the game with prison sentence payoffs and gave it the name &quot;prisoner's dilemma&quot;(Poundstone, 1992). A classic example of the game is presented as follows:

:Two men are arrested, but the police do not have enough information for a conviction. The police separate the two men, and offer both the same deal: if one testifies against his partner (defects/betrays), and the other remains silent (cooperates with/assists his partner), the betrayer goes free and the one that remains silent gets a one-year sentence. If both remain silent, both are sentenced to only one month in jail on a minor charge. If each 'rats out' the other, each receives a three-month sentence. Each prisoner must choose either to betray or remain silent; the decision of each is kept secret from his partner. What should they do? If it is assumed that each player is only concerned with lessening his own time in jail, the game becomes a non-[[zero sum game]] where the two players may either assist or betray the other. The sole concern of the prisoners seems to be increasing his own reward. The interesting symmetry of this problem is that the optimal decision for each is to betray the other, even though they would be better off if they both cooperated. 

In the classic version of the game, collaboration is dominated by betrayal (i.e. betrayal always produces a better outcome) and so the only possible outcome is for both prisoners to betray the other. Regardless of what the other prisoner chooses, one will always gain a greater payoff by betraying the other. Because betrayal is always more beneficial than cooperation, all purely rational prisoners would seemingly betray the other. However, in reality humans display a systematic bias towards cooperative behavior in this and similar games, much more so than predicted by a theory based only on rational self-interested action.&lt;ref&gt;Fehr E, Fischbacher U. 2003. The nature of human altruism. Nature 425:785–791.&lt;/ref&gt;&lt;ref&gt;Tversky A. 2004. Preference, belief, and similarity: selected writings. Cambridge: MIT Press.&lt;/ref&gt;&lt;ref&gt;Ahn TK, Ostrom E, Walker J. 2003. Incorporating motivational heterogeneity into game theoretic models of collective action. Public Choice 117:295–314.&lt;/ref&gt;&lt;ref&gt;Oosterbeek H, Sloof R, van de Kuilen G. 2004. Differences in ultimatum game experiments: evidence from a meta-analysis. Exp Econ 7:171–188.&lt;/ref&gt;&lt;ref&gt;Camerer C. 2003. Behavioral game theory. Princeton: Princeton University Press.&lt;/ref&gt;

There is also an extended &quot;iterative&quot; version of the game, where the classic game is played over and over, and consequently, both prisoners continuously have an opportunity to penalize the other for previous decisions. If the number of times the game will be played is known, the finite aspect of the game means that (by backward induction) the two prisoners will betray each other repeatedly.

In casual usage, the label &quot;prisoner's dilemma&quot; may be applied to situations not strictly matching the formal criteria of the classic or iterative games: for instance, those in which two entities could gain important benefits from cooperating or suffer from the failure to do so, but find it merely difficult or expensive, not necessarily impossible, to coordinate their activities to achieve cooperation.

==Strategy for the classic prisoners' dilemma==

The normal game is shown below:

{| class=&quot;wikitable&quot;
|-
! !! Prisoner B stays silent (''cooperates'') !! Prisoner B betrays (''defects'')
|-
! Prisoner A stays silent (''cooperates'')
| Each serves 1 month|| Prisoner A: 12 months&lt;br/&gt;Prisoner B: goes free
|-
! Prisoner A betrays (''defects'')
| Prisoner A: goes free&lt;br/&gt;Prisoner B: 12 months || Each serves 3 months
|}

Here, regardless of what the other decides, each prisoner gets a higher pay-off by betraying the other. For example, Prisoner A can (according to the payoffs above) state that no matter what prisoner B chooses, prisoner A is better off 'ratting him out' (defecting) than staying silent (cooperating). 
As a result, based on the payoffs above, prisoner A should logically betray him. The game is symmetric, so Prisoner B should act the same way. Since both rationally decide to defect, each receives a lower reward than if both were to stay quiet. Traditional game theory results in both players being worse off than if each chose to lessen the sentence of his accomplice at the cost of spending more time in jail himself.

==Generalized form==
The structure of the traditional Prisoners’ Dilemma can be analyzed by removing its original prisoner setting. Suppose that the two players are represented by colors, red and blue, and that each player chooses to either &quot;Cooperate&quot; or &quot;Defect&quot;. 

If both players play &quot;Cooperate&quot; they both get the payoff ''A''. If Blue plays &quot;Defect&quot; while Red plays &quot;Cooperate&quot; then Blue gets ''B'' while Red gets ''C''. Symmetrically, if Blue plays &quot;Cooperate&quot; while Red plays &quot;Defect&quot; then Blue gets payoff ''C'' while Red gets payoff ''B''. If both players play &quot;Defect&quot; they both get the payoff ''D''. 

In terms of general point values:

{| class=&quot;wikitable&quot;
|+ Canonical PD payoff matrix
|
!scope=&quot;col&quot; style=&quot;color: #900&quot;|Cooperate
!scope=&quot;col&quot; style=&quot;color: #900&quot;|Defect
|-
!scope=&quot;row&quot; style=&quot;color: #009&quot;|Cooperate
|&lt;span style=&quot;color: #009&quot;&gt;A&lt;/span&gt;, &lt;span style=&quot;color: #900&quot;&gt;A&lt;/span&gt;
|&lt;span style=&quot;color: #009&quot;&gt;C&lt;/span&gt;, &lt;span style=&quot;color: #900&quot;&gt;B&lt;/span&gt;
|-
!scope=&quot;row&quot; style=&quot;color: #009&quot;|Defect
|&lt;span style=&quot;color: #009&quot;&gt;B&lt;/span&gt;, &lt;span style=&quot;color: #900&quot;&gt;C&lt;/span&gt;
|&lt;span style=&quot;color: #009&quot;&gt;D&lt;/span&gt;, &lt;span style=&quot;color: #900&quot;&gt;D&lt;/span&gt;
|}

To be a prisoner's dilemma, the following must be true:

''B'' &gt; ''A'' &gt; ''D'' &gt; ''C''

The fact that ''A''&gt;''D'' implies that the &quot;Both Cooperate&quot; outcome is better than the &quot;Both Defect&quot; outcome, while  ''B''&gt;''A'' and ''D''&gt;''C'' imply that &quot;Defect&quot; is the [[dominant strategy]] for both agents.

It is not necessary for a Prisoner's Dilemma to be strictly symmetric as in the above example, merely that the choices which are individually optimal result in an equilibrium which is socially inferior.

==The iterated prisoners' dilemma==
If two players play prisoners' dilemma more than once in succession and they remember previous actions of their opponent and change their strategy accordingly, the game is called iterated prisoners' dilemma.

In addition to the general form above, the iterative version also requires that 2A &gt; B + C, to prevent alternating cooperation and defection giving a greater reward than mutual cooperation.

The iterated prisoners' dilemma game is fundamental to certain theories of human cooperation and trust. On the assumption that the game can model transactions between two people requiring trust, cooperative behaviour in populations may be modeled by a multi-player, iterated, version of the game. It has, consequently, fascinated many scholars over the years. In 1975, Grofman and Pool estimated the count of scholarly articles devoted to it at over 2,000. The iterated prisoners' dilemma has also been referred to as the &quot;[[Peace war game|Peace-War game]]&quot;.&lt;ref&gt;Shy, O., 1996, ''[[industrial organization|Industrial Organization]]: Theory and Applications'', Cambridge, Mass.: The [[MIT]] Press.&lt;/ref&gt;

If the game is played exactly N times and both players know this, then it is always game theoretically optimal to defect in all rounds. The only possible [[Nash equilibrium]] is to always defect. The proof is [[Mathematical induction|inductive]]: one might as well defect on the last turn, since the opponent will not have a chance to punish the player. Therefore, both will defect on the last turn. Thus, the player might as well defect on the second-to-last turn, since the opponent will defect on the last no matter what is done, and so on.  The same applies if the game length is unknown but has a known upper limit.

Unlike the standard prisoners' dilemma, in the iterated prisoners' dilemma the defection strategy is counter-intuitive and fails badly to predict the behavior of human players. Within standard economic theory, though, this is the only correct answer. The [[superrational]] strategy in the iterated prisoners' dilemma with fixed N is to cooperate against a superrational opponent, and in the limit of large N, experimental results on strategies agree with the superrational version, not the game-theoretic rational one.

For cooperation to emerge between game theoretic rational players, the total number of rounds N must be random, or at least unknown to the players. In this case always defect may no longer be a strictly dominant strategy, only a Nash equilibrium. Amongst results shown by [[Robert Aumann]] in a 1959 paper, rational players repeatedly interacting for indefinitely long games can sustain the cooperative outcome.

===Strategy for the iterated prisoners' dilemma===
Interest in the iterated prisoners' dilemma (IPD) was kindled by [[Robert Axelrod]] in his book ''[[The Evolution of Cooperation]]'' (1984). In it he reports on a tournament he organized of the N step prisoners' dilemma (with N fixed) in which participants have to choose their mutual strategy again and again, and have memory of their previous encounters. Axelrod invited academic colleagues all over the world to devise computer strategies to compete in an IPD tournament. The programs that were entered varied widely in algorithmic complexity, initial hostility, capacity for forgiveness, and so forth.

Axelrod discovered that when these encounters were repeated over a long period of time with many players, each with different strategies, greedy strategies tended to do very poorly in the long run while more [[altruism|altruistic]] strategies did better, as judged purely by self-interest. He used this to show a possible mechanism for the evolution of altruistic behaviour from mechanisms that are initially purely selfish, by [[natural selection]].

The best [[deterministic algorithm|deterministic]] strategy was found to be [[tit for tat]], which [[Anatol Rapoport]] developed and entered into the tournament. It was the simplest of any program entered, containing only four lines of BASIC, and won the contest. The strategy is simply to cooperate on the first iteration of the game; after that, the player does what his or her opponent did on the previous move. Depending on the situation, a slightly better strategy can be &quot;tit for tat with forgiveness.&quot; When the opponent defects, on the next move, the player sometimes cooperates anyway, with a small probability (around 1–5%). This allows for occasional recovery from getting trapped in a cycle of defections. The exact probability depends on the line-up of opponents.

By analysing the top-scoring strategies, Axelrod stated several conditions necessary for a strategy to be successful.

; Nice: The most important condition is that the strategy must be &quot;nice&quot;, that is, it will not defect before its opponent does (this is sometimes referred to as an &quot;optimistic&quot; algorithm). Almost all of the top-scoring strategies were nice; therefore, a purely selfish strategy will not &quot;cheat&quot; on its opponent, for purely self-interested reasons first.
; Retaliating: However, Axelrod contended, the successful strategy must not be a blind optimist. It must sometimes retaliate. An example of a non-retaliating strategy is Always Cooperate. This is a very bad choice, as &quot;nasty&quot; strategies will ruthlessly exploit such players.
; Forgiving: Successful strategies must also be forgiving. Though players will retaliate, they will once again fall back to cooperating if the opponent does not continue to defect. This stops long runs of revenge and counter-revenge, maximizing points.
; Non-envious: The last quality is being non-envious, that is not striving to score more than the opponent (note that a &quot;nice&quot; strategy can never score more than the opponent).

The optimal (points-maximizing) strategy for the one-time PD game is simply defection; as explained above, this is true whatever the composition of opponents may be. However, in the iterated-PD game the optimal strategy depends upon the strategies of likely opponents, and how they will react to defections and cooperations. For example, consider a population where everyone defects every time, except for a single individual following the tit for tat strategy. That individual is at a slight disadvantage because of the loss on the first turn. In such a population, the optimal strategy for that individual is to defect every time. In a population with a certain percentage of always-defectors and the rest being tit for tat players, the optimal strategy for an individual depends on the percentage, and on the length of the game.

A strategy called Pavlov (an example of [[Win-Stay, Lose-Switch]]) cooperates at the first iteration and whenever the player and co-player did the same thing at the previous iteration; Pavlov defects when the player and co-player did different things at the previous iteration. For a certain range of parameters, Pavlov beats all other strategies by giving preferential treatment to co-players which resemble Pavlov.

Deriving the optimal strategy is generally done in two ways:
# [[Bayesian Nash equilibrium|Bayesian Nash Equilibrium]]: If the statistical distribution of opposing strategies can be determined (e.g. 50% tit for tat, 50% always cooperate) an optimal counter-strategy can be derived analytically.&lt;ref name=&quot;bne&quot;&gt;For example see the 2003 study [http://econ.hevra.haifa.ac.il/~mbengad/seminars/whole1.pdf  “Bayesian Nash equilibrium; a statistical test of the hypothesis”] for discussion of the concept and whether it can apply in real [[economic]] or strategic situations (from [[Tel Aviv University]]).&lt;/ref&gt;
# [[Monte Carlo method|Monte Carlo]] simulations of populations have been made, where individuals with low scores die off, and those with high scores reproduce (a [[genetic algorithm]] for finding an optimal strategy). The mix of algorithms in the final population generally depends on the mix in the initial population. The introduction of mutation (random variation during reproduction) lessens the dependency on the initial population; empirical experiments with such systems tend to produce tit for tat players (see for instance Chess 1988), but there is no analytic proof that this will always occur.

Although tit for tat is considered to be the most robust basic strategy, a team from [[Southampton University]] in England (led by Professor Nicholas Jennings [http://www.ecs.soton.ac.uk/~nrj] and consisting of Rajdeep Dash, Sarvapali Ramchurn, Alex Rogers, Perukrishnen Vytelingum) introduced a new strategy at the 20th-anniversary iterated prisoners' dilemma competition, which proved to be more successful than tit for tat. This strategy relied on cooperation between programs to achieve the highest number of points for a single program. The University submitted 60 programs to the competition, which were designed to recognize each other through a series of five to ten moves at the start. Once this recognition was made, one program would always cooperate and the other would always defect, assuring the maximum number of points for the defector. If the program realized that it was playing a non-Southampton player, it would continuously defect in an attempt to minimize the score of the competing program. As a result,&lt;ref name=&quot;southamptontrick&quot;&gt;[http://www.prisoners'-dilemma.com/results/cec04/ipd_cec04_full_run.html The 2004 Prisoners' Dilemma Tournament Results] show [[University of Southampton]]'s strategies in the first three places, despite having fewer wins and many more losses than the GRIM strategy. (Note that in a PD tournament, the aim of the game is not to “win” matches&amp;nbsp;— that can easily be achieved by frequent defection). It should also be pointed out that even without implicit collusion between [[computer program|software strategies]] (exploited by the Southampton team) tit for tat is not always the absolute winner of any given tournament; it would be more precise to say that its long run results over a series of tournaments outperform its rivals. (In any one event a given strategy can be slightly better adjusted to the competition than tit for tat, but tit for tat is more robust). The same applies for the tit for tat with forgiveness variant, and other optimal strategies: on any given day they might not 'win' against a specific mix of counter-strategies.An alternative way of putting it is using the Darwinian [[Evolutionarily stable strategy|ESS]] simulation. In such a simulation, tit for tat will almost always come to dominate, though nasty strategies will drift in and out of the population because a tit for tat population is penetrable by non-retaliating nice strategies, which in turn are easy prey for the nasty strategies. Richard Dawkins showed that here, no static mix of strategies form a stable equilibrium and the system will always oscillate between bounds.&lt;/ref&gt; this strategy ended up taking the top three positions in the competition, as well as a number of positions towards the bottom.

This strategy takes advantage of the fact that multiple entries were allowed in this particular competition and that the performance of a team was measured by that of the highest-scoring player (meaning that the use of self-sacrificing players was a form of [[minmaxing]]). In a competition where one has control of only a single player, tit for tat is certainly a better strategy. Because of this new rule, this competition also has little theoretical significance when analysing single agent strategies as compared to Axelrod's seminal tournament. However, it provided the framework for analysing how to achieve cooperative strategies in multi-agent frameworks, especially in the presence of noise. In fact, long before this new-rules tournament was played, [[Richard Dawkins]] in his book ''[[The Selfish Gene]]'' pointed out the possibility of such strategies winning if multiple entries were allowed, but he remarked that most probably Axelrod would not have allowed them if they had been submitted. It also relies on circumventing rules about the prisoners' dilemma in that there is no communication allowed between the two players. When the Southampton programs engage in an opening &quot;ten move dance&quot; to recognize one another, this only reinforces just how valuable communication can be in shifting the balance of the game.

===Continuous iterated prisoners' dilemma===
Most work on the iterated prisoners' dilemma has focused on the discrete case, in which players either cooperate or defect, because this model is relatively simple to analyze. However, some researchers have looked at models of the continuous iterated prisoners' dilemma, in which players are able to make a variable contribution to the other player. Le and Boyd&lt;ref&gt;
Le, S. and R. Boyd (2007) &quot;Evolutionary Dynamics of the Continuous Iterated Prisoner's Dilemma&quot; Journal of Theoretical Biology, Volume 245, 258–267.&lt;/ref&gt; found that in such situations, cooperation is much harder to evolve than in the discrete iterated prisoners' dilemma. The basic intuition for this result is straightforward: in a continuous prisoners' dilemma, if a population starts off in a non-cooperative equilibrium, players who are only marginally more cooperative than non-cooperators get little benefit from assorting with one another. By contrast, in a discrete prisoners' dilemma, tit for tat cooperators get a big payoff boost from assorting with one another in a non-cooperative equilibrium, relative to non-cooperators. Since nature arguably offers more opportunities for variable cooperation rather than a strict dichotomy of cooperation or defection, the continuous prisoners' dilemma may help explain why real-life examples of tit for tat-like cooperation are extremely rare in nature (ex. Hammerstein&lt;ref&gt;Hammerstein, P. (2003). Why is reciprocity so rare in social animals? A protestant appeal. In: P. Hammerstein, Editor, Genetic and Cultural Evolution of Cooperation, MIT Press. pp. 83–94.
&lt;/ref&gt;) even though tit for tat seems robust in theoretical models.

==Real-life examples==
These particular examples, involving prisoners and bag switching and so forth, may seem contrived, but there are in fact many examples in human interaction as well as interactions in nature that have the same payoff matrix. The prisoner's dilemma is therefore of interest to the [[social science]]s such as [[economics]], [[politics]] and [[sociology]], as well as to the biological sciences such as [[ethology]] and [[evolutionary biology]]. Many natural processes have been abstracted into models in which living beings are engaged in endless games of prisoner's dilemma. This wide applicability of the PD gives the game its substantial importance.

===In environmental studies===

In [[environmental studies]], the PD is evident in crises such as global [[climate change]]. It is argued all countries will benefit from a stable climate, but any single country is often hesitant to curb [[Carbon dioxide|{{Co2}}]] emissions. The immediate benefit to an individual country to maintain current behavior is perceived to be greater than the purported eventual benefit to all countries if behavior was changed, therefore explaining the current impasse concerning climate change.&lt;ref&gt;{{cite news |newspaper=[[The Economist]] |year=2007 |url=http://www.economist.com/finance/displaystory.cfm?story_id=9867020 | title=Markets &amp; Data |date=2007-09-27}}&lt;/ref&gt;

===In psychology===
In [[addiction]] research/[[behavioral economics]], [[George Ainslie (psychologist)|George Ainslie]] points out&lt;ref&gt;{{cite book |author=George Ainslie |title=Breakdown of Will |year=2001 |isbn=0-521-59694-7}}&lt;/ref&gt; that addiction can be cast as an intertemporal PD problem between the present and future selves of the addict.  In this case, ''defecting'' means ''relapsing'', and it is easy to see that not defecting both today and in the future is by far the best outcome, and that defecting both today and in the future is the worst outcome.  The case where one abstains today but relapses in the future is clearly a bad outcome—in some sense the discipline and self-sacrifice involved in abstaining today have been &quot;wasted&quot; because the future relapse means that the addict is right back where he started and will have to start over (which is quite demoralizing, and makes starting over more difficult).  The final case, where one engages in the addictive behavior today while abstaining &quot;tomorrow&quot; will be familiar to anyone who has struggled with an addiction.  The problem here is that (as in other PDs) there is an obvious benefit to defecting &quot;today&quot;, but tomorrow one will face the same PD, and the same obvious benefit will be present then, ultimately leading to an endless string of defections.

[[John Gottman]] in his research described in &quot;the science of trust&quot; defines good relationships as those where partners know not to enter the (D,D) cell or at least not to get dynamically stuck there in a loop.

===In economics===

Advertising is sometimes cited as a real life example of the prisoner’s dilemma.  When [[cigarette advertising]] was legal in the United States, competing cigarette manufacturers had to decide how much money to spend on advertising.  The effectiveness of Firm A’s advertising was partially determined by the advertising conducted by Firm B.  Likewise, the profit derived from advertising for Firm B is affected by the advertising conducted by Firm A.  If both Firm A and Firm B chose to advertise during a given period the advertising cancels out, receipts remain constant, and expenses increase due to the cost of advertising.  Both firms would benefit from a reduction in advertising.  However, should Firm B choose not to advertise, Firm A could benefit greatly by advertising. Nevertheless, the optimal amount of advertising by one firm depends on how much advertising the other undertakes. As the best strategy is dependent on what the other firm chooses there is no dominant strategy, which makes it slightly different than a prisoner's dilemma. The outcome is similar, though, in that both firms would be better off were they to advertise less than in the equilibrium. Sometimes cooperative behaviors do emerge in business situations.  For instance, cigarette manufacturers endorsed the creation of laws banning cigarette advertising, understanding that this would reduce costs and increase profits across the industry.&lt;ref name=&quot;trust&quot;&gt;This argument for the development of cooperation through trust is given in '' [[The Wisdom of Crowds]] '', where it is argued that long-distance [[capitalism]] was able to form around a nucleus of [[Religious Society of Friends|Quaker]]s, who always dealt honourably with their business partners. (Rather than defecting and reneging on promises&amp;nbsp;— a phenomenon that had discouraged earlier long-term unenforceable overseas contracts). It is argued that dealings with reliable merchants allowed the [[meme]] for cooperation to spread to other traders, who spread it further until a high degree of cooperation became a profitable strategy in general [[commerce]]&lt;/ref&gt;  This analysis is likely to be pertinent in many other business situations involving advertising.

Another example of the prisoner's dilemma in economics is competition-oriented objectives. &lt;ref&gt;{{cite journal|url=http://marketing.wharton.upenn.edu/documents/research/CompOrientPDF%2011-27%20%282%29.pdf | title = Competitor-oriented Objectives: The Myth of Market Share | author = J. Scott Armstrong and Kesten C. Greene | journal = International Journal of Business | volume = 12 | issue = 1 | pages = 116–134 | year = 2007 | ISBN 1083-4346}}&lt;/ref&gt; When firms are aware of the activities of their competitors, they tend to pursue policies that are designed to oust their competitors as opposed to maximizing the performance of the firm. This approach impedes the firm from functioning at its maximum capacity because it limits the scope of the strategies employed by the firms.

Without enforceable agreements, members of a [[cartel]] are also involved in a (multi-player) prisoners' dilemma.&lt;ref name=NicholsonIntermediateMicroEd8&gt;{{Cite document|last1=Nicholson|first=Walter|authorlink=Walter Nicholson|year=2000|title=Intermediate Microeconomics|edition=8th|publisher=Harcourt}}&lt;/ref&gt; 'Cooperating' typically means keeping prices at a pre-agreed minimum level. 'Defecting' means selling under this minimum level, instantly taking business (and profits) from other cartel members. [[Anti-trust]] authorities want potential cartel members to mutually defect, ensuring the lowest possible prices for [[consumer]]s.

===In sport===

[[Doping in sport]] has been cited as an example of a prisoner's dilemma.&lt;ref&gt;{{cite web|last=Schneier |first=Bruce |url=http://www.wired.com/opinion/2012/10/lance-armstrong-and-the-prisoners-dilemma-of-doping-in-professional-sports/ |title=Lance Armstrong and the Prisoners' Dilemma of Doping in Professional Sports &amp;#124; Wired Opinion |publisher=Wired.com |date=1948-10-26 |accessdate=2012-10-29}}&lt;/ref&gt;

If two competing athletes have the option to use an illegal and dangerous drug to boost their performance, then they must also consider the likely behaviour of their competitor. If neither athlete takes the drug, then neither gains an advantage. If only one does, then that athlete gains a significant advantage over their competitor (reduced only by the legal or medical dangers of having taken the drug). If both athletes take the drug, however, the benefits cancel out and only the drawbacks remain, putting them both in a worse position than if neither had used doping.&lt;ref&gt;{{cite web|last=Schneier |first=Bruce|url=http://www.wired.com/opinion/2012/10/lance-armstrong-and-the-prisoners-dilemma-of-doping-in-professional-sports/ |title=Lance Armstrong and the Prisoners' Dilemma of Doping in Professional Sports &amp;#124; Wired Opinion |publisher=Wired.com |date=1948-10-26 |accessdate=2012-10-29}}&lt;/ref&gt;

===Multiplayer dilemmas===

Many real-life dilemmas involve multiple players. Although metaphorical, [[Garrett Hardin|Hardin's]] [[tragedy of the commons]] may be viewed as an example of a multi-player generalization of the PD: Each villager makes a choice for personal gain or restraint. The collective reward for unanimous (or even frequent) defection is very low payoffs (representing the destruction of the &quot;commons&quot;). The commons are not always exploited: [[William Poundstone]], in a book about the prisoner's dilemma (see References below), describes a situation in New Zealand where newspaper boxes are left unlocked. It is possible for people to [[Excludability|take a paper without paying]] (''defecting'') but very few do, feeling that if they do not pay then neither will others, destroying the system. Subsequent research by [[Elinor Ostrom]], winner of the 2009 Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, hypothesized that the [[tragedy of the commons]] is oversimplified, with the negative outcome influenced by outside influences. Without complicating pressures, groups communicate and manage the commons among themselves for their mutual benefit, enforcing social norms to preserve the resource and achieve the maximum good for the group, an example of effecting the best case outcome for PD.&lt;ref&gt;{{cite web|author=Česky |url=http://en.wikipedia.org/wiki/Tragedy_of_the_commons |title=Tragedy of the commons - Wikipedia, the free encyclopedia |publisher=En.wikipedia.org |date= |accessdate=2011-12-17}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://volokh.com/2009/10/12/elinor-ostrom-and-the-tragedy-of-the-commons/ |title=The Volokh Conspiracy » Elinor Ostrom and the Tragedy of the Commons |publisher=Volokh.com |date=2009-10-12 |accessdate=2011-12-17}}&lt;/ref&gt;

===The Cold War===

The [[Cold War]] and similar arms races can be modelled as a Prisoner's Dilemma situation.&lt;ref&gt;{{cite journal|url=http://www.sciencedirect.com/science/article/pii/0165489684900222 | title = Arms races as iterated prisoner's dilemma games | author = Stephen J. Majeski | journal = Mathematical and Social Sciences | volume = 7 | issue = 3 | pages = 253-266 | year = 1984 }}&lt;/ref&gt; During the Cold War the opposing alliances of [[NATO]] and the [[Warsaw Pact]] both had the choice to arm or disarm. From each side's point of view: Disarming whilst your opponent continues to arm would have led to military inferiority and possible annihilation. If both sides chose to arm, neither could afford to attack each other, but at the high cost of maintaining and developing a nuclear arsenal. If both sides chose to disarm, war would be avoided and there would be no costs. If your opponent disarmed while you continue to arm, then you achieve superiority. 

Although the 'best' overall outcome is for both sides to disarm, the rational course for both sides is to arm. This is indeed what happened, and both sides poured enormous resources in to military research and armament for the next thirty years until the [[dissolution of the Soviet Union]] broke the deadlock.

==Related games==
===Closed-bag exchange===
[[Douglas Hofstadter|Hofstadter]]&lt;ref name=&quot;dh&quot;&gt;{{cite book | first=Douglas R. | last=Hofstadter| authorlink=Douglas Hofstadter | title= [[Metamagical Themas]]: questing for the essence of mind and pattern | publisher= Bantam Dell Pub Group| year=1985 | isbn=0-465-04566-9}} – see Ch.29 ''The Prisoner's Dilemma Computer Tournaments and the Evolution of Cooperation''.&lt;/ref&gt; once suggested that people often find problems such as the PD problem easier to understand when it is illustrated in the form of a simple game, or trade-off. One of several examples he used was &quot;closed bag exchange&quot;:
: Two people meet and exchange closed bags, with the understanding that one of them contains money, and the other contains a purchase. Either player can choose to honor the deal by putting into his or her bag what he or she agreed, or he or she can defect by handing over an empty bag.

In this game, defection is always the best course, implying that rational agents will never play. However, in this case both players cooperating and both players defecting actually give the same result, assuming there are no [[gains from trade]], so chances of mutual cooperation, even in repeated games, are few.

===''Friend or Foe?''===
''[[Friend or Foe?]]'' is a game show that aired from 2002 to 2005 on the [[Game Show Network]] in the [[United States]]. It is an example of the prisoner's dilemma game tested on real people, but in an artificial setting. On the game show, three pairs of people compete. When a pair is eliminated, they play a game similar to the prisoner's dilemma to determine how the winnings are split. If they both cooperate (Friend), they share the winnings 50–50. If one cooperates and the other defects (Foe), the defector gets all the winnings and the cooperator gets nothing. If both defect, both leave with nothing. Notice that the payoff matrix is slightly different from the standard one given above, as the payouts for the &quot;both defect&quot; and the &quot;cooperate while the opponent defects&quot; cases are identical. This makes the &quot;both defect&quot; case a weak equilibrium, compared with being a strict equilibrium in the standard prisoner's dilemma. If you know your opponent is going to vote Foe, then your choice does not affect your winnings. In a certain sense, ''Friend or Foe'' has a payoff model between prisoner's dilemma and the [[Chicken (game)|game of Chicken]].

The payoff matrix is
{| class=&quot;wikitable&quot;
|
!scope=&quot;col&quot; style=&quot;color: #900&quot;|Cooperate
!scope=&quot;col&quot; style=&quot;color: #900&quot;|Defect
|-
!scope=&quot;row&quot; style=&quot;color: #009&quot;|Cooperate
|&lt;span style=&quot;color: #009&quot;&gt;1&lt;/span&gt;, &lt;span style=&quot;color: #900&quot;&gt;1
|&lt;span style=&quot;color: #009&quot;&gt;0&lt;/span&gt;, &lt;span style=&quot;color: #900&quot;&gt;2
|-
!scope=&quot;row&quot; style=&quot;color: #009&quot;|Defect
|&lt;span style=&quot;color: #009&quot;&gt;2&lt;/span&gt;, &lt;span style=&quot;color: #900&quot;&gt;0&lt;/span&gt;
|&lt;span style=&quot;color: #009&quot;&gt;0&lt;/span&gt;, &lt;span style=&quot;color: #900&quot;&gt;0&lt;/span&gt;
|}

This payoff matrix has also been used on the [[United Kingdom|British]] [[television]] programmes ''Trust Me'', ''[[Shafted]]'', ''[[The Bank Job (TV series)|The Bank Job]]'' and ''[[Golden Balls]]'', and on the [[United States|American]] show ''[[Bachelor Pad]]''. Game data from the ''[[Golden Balls]]'' series has been analyzed by a team of economists, who found that cooperation was &quot;surprisingly high&quot; for amounts of money that would seem consequential in the real world, but were comparatively low in the context of the game.&lt;ref&gt;{{cite journal | url=http://ssrn.com/abstract=1592456 | title=Split or Steal? Cooperative Behavior When the Stakes Are Large | author=Van den Assem, Martijn J. | journal=Management Science | year=2012 | month=January | volume=58 | issue=1 | pages=2-20}}&lt;/ref&gt;

===Iterated snowdrift===
A modified version of the PD modifies the payoff matrix to reduce the risk of cooperation in the case of partner defection.  This may better reflect real world scenarios: &quot;For example two scientists collaborating on a report would benefit if the other worked harder. But when your collaborator doesn’t do any work, it’s probably better for you to do all the work yourself. You’ll still end up with a completed project.&quot;&lt;ref&gt;{{cite web|last=Kümmerli|first=Rolf|title='Snowdrift' game tops 'Prisoner's Dilemma' in explaining cooperation|url=http://phys.org/news111145481.html|accessdate=11 April 2012}}&lt;/ref&gt;

{| class=&quot;wikitable&quot; style=&quot;text-align: center; width: 400px;&quot;
|+ Example Snowdrift Payouts (A, B)
|-
! !! A cooperates !! A defects
|-
! B cooperates
| 200, 200 || 300, 100
|-
! B defects
| 100, 300 || 0, 0
|}

{| class=&quot;wikitable&quot; style=&quot;text-align: center; width: 400px;&quot;
|+ Example PD Payouts (A, B)
|-
! !! A cooperates !! A defects
|-
! B cooperates
| 200, 200 || 300, -100
|-
! B defects
| -100, 300 || 0, 0
|}

==See also==
&lt;div style=&quot;-moz-column-count:4; column-count:4;&quot;&gt;
* [[Centipede game]]
* [[Christmas truce]]
* [[Cooperation]]
* [[Diner's dilemma]]
* [[Ethical dilemma]]
* [[Evolutionarily stable strategy]]
* [[Folk theorem (game theory)]]
* [[Innocent prisoner's dilemma]]
* [[Nash equilibrium]]
* [[Robert H. Frank#Prisoner's dilemma and cooperation|Prisoner's dilemma and cooperation]] an experimental study
* [[Platonia dilemma]]
* [[Public choice theory]]
* [[Public goods game]]
* [[Reciprocal altruism]]
* [[Rendezvous problem]]
* [[Simultaneous action selection]]
* [[Social trap]]
* [[Superrationality]]: an attempt to improve on the traditional game theory approach.
* [[Tit for tat]]
* [[Tragedy of the commons]]
* [[Traveler's dilemma]]
* [[War of attrition (game)]]
* [[Zero-sum]]
&lt;/div&gt;

==Notes==
&lt;!--This article uses the Cite.php citation mechanism. If you would like more information on how to add references to this article, please see http://meta.wikimedia.org/wiki/Cite/Cite.php --&gt;
{{Reflist|2}}

==References==
{{Refbegin}}
* [[Robert Aumann]], “Acceptable points in general cooperative n-person games”, in R. D. Luce and A. W. Tucker (eds.), Contributions to the Theory 23 of Games IV, Annals of Mathematics Study 40, 287–324, Princeton University Press, Princeton NJ.
* [[Robert Axelrod|Axelrod, R.]] (1984). ''[[The Evolution of Cooperation]]''. ISBN 0-465-02121-2
* [[Cristina Bicchieri|Bicchieri, Cristina]] (1993). Rationality and Coordination. [[Cambridge University Press]].
* [[Kenneth Binmore]], Fun and Games.
* David M. Chess (1988). Simulating the evolution of behavior: the iterated prisoners' dilemma problem. Complex Systems, 2:663–670.
* [[Melvin Dresher|Dresher, M.]] (1961). ''The Mathematics of Games of Strategy: Theory and Applications''  [[Prentice-Hall]], Englewood Cliffs, NJ.
* [[Merrill M. Flood|Flood, M.M.]] (1952). Some experimental games. Research memorandum RM-789. [[RAND]] Corporation, Santa Monica, CA. &lt;!--(Research Memoranda do not appear for sale at the RAND [http://www.rand.org/pubs/authors/f/flood_merrill_m.html store])--&gt;
* Kaminski, Marek M. (2004) [http://webfiles.uci.edu/mkaminsk/www/book.html ''Games Prisoners Play''] [[Princeton University Press]]. ISBN 0-691-11721-7
* Poundstone, W. (1992) ''Prisoner's Dilemma'' [[Doubleday (publisher)|Doubleday]], NY NY.
* Greif, A. (2006). ''Institutions and the Path to the Modern Economy: Lessons from Medieval Trade.'' Cambridge University Press, [[Cambridge]], UK.
* [[Anatol Rapoport|Rapoport, Anatol]] and Albert M. Chammah (1965). ''Prisoner's Dilemma''. [[University of Michigan Press]].
* S. Le and R. Boyd (2007) &quot;Evolutionary Dynamics of the Continuous Iterated Prisoner's Dilemma&quot; Journal of Theoretical Biology, Volume 245, 258–267. [http://letuhuy.bol.ucla.edu/academic/cont_ipd_Le_Boyd_JTB.pdf Full text]
* A. Rogers, R. K. Dash, S. D. Ramchurn, P. Vytelingum and N. R. Jennings (2007)  “[http://users.ecs.soton.ac.uk/nrj/download-files/tcs07.pdf Coordinating team players within a noisy iterated Prisoner’s Dilemma tournament]”  Theoretical Computer Science 377 (1–3) 243–259.
* M.J. van den Assem, D. van Dolder and R.H. Thaler (2010). [http://ssrn.com/abstract=1592456 ''&quot;Split or Steal? Cooperative Behavior When the Stakes are Large&quot;'']
{{Refend}}

==Further reading==
* [[Cristina Bicchieri|Bicchieri, Cristina]] and Mitchell Green (1997) &quot;Symmetry Arguments for Cooperation in the Prisoner's Dilemma&quot;, in G. Holmstrom-Hintikka and R. Tuomela (eds.), Contemporary Action Theory: The Philosophy and Logic of Social Action, Kluwer.
* [http://aleph0.clarku.edu/~djoyce/Moth/webrefs.html ''Iterated Prisoner's Dilemma Bibliography web links''], July, 2005.
* Plous, S. (1993). Prisoner's Dilemma or Perceptual Dilemma? ''Journal of Peace Research'', Vol. 30, No. 2, 163–179.

==External links==
{{Spoken Wikipedia|Prisoners_Dilemma.ogg|2007-06-25}}
*[http://www.pnas.org/content/early/2012/05/16/1206569109.full.pdf+html Iterated Prisoner’s Dilemma contains strategies that dominate any evolutionary opponent]
*[http://plato.stanford.edu/entries/prisoner-dilemma/ Prisoner's Dilemma (''Stanford Encyclopedia of Philosophy'')]
*[http://www.nature.com/npp/journal/v31/n5/full/1300932a.html Effects of Tryptophan Depletion on the Performance of an Iterated Prisoner's Dilemma Game in Healthy Adults] – Nature Neuropsychopharmacology
*[http://www.egwald.ca/operationsresearch/prisonersdilemma.php Is there a &quot;dilemma&quot; in Prisoner's Dilemma?] by Elmer G. Wiens
*[http://webfiles.uci.edu/mkaminsk/www/book.html &quot;Games Prisoners Play&quot;] – game-theoretic analysis of interactions among actual prisoners, including PD.
*[http://www.iterated-prisoners-dilemma.net/ Iterated prisoner's dilemma game]
*Another version of the [http://kane.me.uk/ipd/ Iterated prisoner's dilemma game]
*Another version of the [http://www.gametheory.net/Web/PDilemma/ Iterated prisoner's dilemma game]
*[http://www.paulspages.co.uk/hmd/ Iterated prisoner's dilemma game] applied to ''Big Brother'' TV show situation.
*[http://www.msri.org/ext/larryg/pages/15.htm The Bowerbird's Dilemma] The Prisoner's Dilemma in ornithology&amp;nbsp;— mathematical cartoon by Larry Gonnick.
*[http://www.economics.li/downloads/egefdile.pdf Examples of Prisoners' dilemma]
*[http://www.gohfgl.com/  Multiplayer game based on prisoner dilemma] Play prisoner's dilemma over IRC &amp;nbsp;— by Axiologic Research.
*[http://fortwain.com/pddg.html Prisoner's Dilemma Party Game] A party game based on the prisoner's dilemma
*[http://www.rte.ie/tv/theview/archive/20080331.html The Edge cites Robert Axelrod's book and discusses the success of U2 following the principles of IPD.]
*[http://arxiv.org/abs/quant-ph/0503233v2 Classical and Quantum Contents of Solvable Game Theory on Hilbert Space]
*{{cite episode
| title        = Radiolab: &quot;The Good Show&quot;
| url          = http://www.radiolab.org/2010/dec/14/
| station      = WNYC
| city         = New York
| airdate      = December 14,2011
| season       = 9
| number       = 1}}


{{Game theory}}

{{DEFAULTSORT:Prisoner's Dilemma}}
[[Category:Game theory]]
[[Category:Thought experiments]]
[[Category:Dilemmas]]

{{Link GA|de}}
{{Link GA|zh}}

[[ar:معضلة السجينين]]
[[bg:Дилема на затворника]]
[[ca:Dilema del presoner]]
[[cs:Vězňovo dilema]]
[[da:Fangernes dilemma]]
[[de:Gefangenendilemma]]
[[et:Vangi dilemma]]
[[es:Dilema del prisionero]]
[[eo:Prizonula Dilemo]]
[[fa:معمای زندانی‌ها]]
[[fr:Dilemme du prisonnier]]
[[ko:죄수의 딜레마]]
[[hi:बंदी की दुविधा]]
[[io:Karcerano-dilemo]]
[[is:Vandamál fangans]]
[[it:Dilemma del prigioniero]]
[[he:דילמת האסיר]]
[[ka:ტუსაღის დილემა]]
[[lv:Cietumnieka dilemma]]
[[lt:Kalinio dilema]]
[[hu:Fogolydilemma]]
[[nl:Prisoner's dilemma]]
[[ja:囚人のジレンマ]]
[[no:Fangens dilemma]]
[[pl:Dylemat więźnia]]
[[pt:Dilema do prisioneiro]]
[[ro:Dilema prizonierului]]
[[ru:Дилемма заключённого]]
[[sk:Väzňova dilema]]
[[sl:Zapornikova dilema]]
[[sr:Затвореникова дилема]]
[[fi:Vangin dilemma]]
[[sv:Fångens dilemma]]
[[ta:கைதியின் குழப்பம்]]
[[th:ความลำบากใจของนักโทษ]]
[[tr:Tutsak ikilemi]]
[[uk:Дилема в'язня]]
[[vi:Song đề tù nhân]]
[[zh-yue:監躉困境]]
[[zh:囚徒困境]]</text>
      <sha1>rt6wl2f2xy11hhclzyun7gf5lu05wyq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
</mediawiki>
