Bellman equation A Bellman equation (also known as a dynamic programming
equation), named after its discoverer, Richard Bellman, is a necessary
condition for optimality associated with the mathematical optimization method
known as dynamic programming. It writes the value of a decision problem at a
certain point in time in terms of the payoff from some initial choices and the
value of the remaining decision problem that results from those initial
choices. This breaks a dynamic optimization problem into simpler subproblems,
as Bellman's Principle of Optimality prescribes. The Bellman equation was first
applied to engineering control theory and to other topics in applied
mathematics, and subsequently became an important tool in economic theory.
Almost any problem which can be solved using optimal control theory can also be
solved by analyzing the appropriate Bellman equation. However, the term
'Bellman equation' usually refers to the dynamic programming equation
associated with discrete-time optimization problems. In continuous-time
optimization problems, the analogous equation is a partial differential
equation which is usually called the Hamilton-Jacobi-Bellman equation.
Analytical concepts in dynamic programming. To understand the Bellman equation,
several underlying concepts must be understood. First, any optimization problem
has some objective--- minimizing travel time, minimizing cost, maximizing
profits, maximizing utility, et cetera. The mathematical function that
describes this objective is called the objective function. Dynamic programming
breaks a multi-period planning problem into simpler steps at different points
in time. Therefore, it requires keeping track of how the decision situation is
evolving over time. The information about the current situation which is needed
to make a correct decision is called the state (See Bellman, 1957, Ch. III.2).
For example, to decide how much to consume and spend at each point in time,
people would need to know (among other things) their initial wealth. Therefore,
wealth would be one of their state variables, but there would probably be
others. The variables chosen at any given point in time are often called the
control variables. For example, given their current wealth, people might decide
how much to consume now. Choosing the control variables now may be equivalent
to choosing the next state; more generally, the next state is affected by other
factors in addition to the current control. For example, in the simplest case,
today's wealth (the state) and consumption (the control) might exactly
determine tomorrow's wealth (the new state), though typically other factors
will affect tomorrow's wealth too. The dynamic programming approach describes
the optimal plan by finding a rule that tells what the controls should be,
given any possible value of the state. For example, if consumption ("c")
depends "only" on wealth ("W"), we would seek a rule formula_1 that gives
consumption as a function of wealth. Such a rule, determining the controls as a
function of the states, is called a policy function (See Bellman, 1957, Ch.
III.2). Finally, by definition, the optimal decision rule is the one that
achieves the best possible value of the objective. For example, if someone
chooses consumption, given wealth, in order to maximize happiness (assuming
happiness "H" can be represented by a mathematical function, such as a utility
function), then each level of wealth will be associated with some highest
possible level of happiness, formula_2. The best possible value of the
objective, written as a function of the state, is called the value function.
Richard Bellman showed that a dynamic optimization problem in discrete time can
be stated in a recursive, step-by-step form by writing down the relationship
between the value function in one period and the value function in the next
period. The relationship between these two value functions is called the
Bellman equation. Deriving the Bellman equation. A dynamic decision problem.
Let the state at time formula_3 be formula_4. For a decision that begins at
time 0, we take as given the initial state formula_5. At any time, the set of
possible actions depends on the current state; we can write this as formula_6,
where the action formula_7 represents one or more control variables. We also
assume that the state changes from formula_8 to a new state formula_9 when
action formula_10 is taken, and that the current payoff from taking action
formula_10 in state formula_8 is formula_13. Finally, we assume impatience,
represented by a discount factor formula_14. subject to the constraints Notice
that we have defined notation formula_17 to represent the optimal value that
can be obtained by maximizing this objective function subject to the assumed
constraints. This function is the "value function". It is a function of the
initial state variable formula_18, since the best value obtainable depends on
the initial situation. Bellman's Principle of Optimality. The dynamic
programming method breaks this decision problem into smaller subproblems.
Richard Bellman's Principle of Optimality describes how to do this:Principle of
Optimality: An optimal policy has the property that whatever the initial state
and initial decision are, the remaining decisions must constitute an optimal
policy with regard to the state resulting from the first decision. (See
Bellman, 1957, Chap. III.3.) In computer science, a problem that can be broken
apart like this is said to have optimal substructure. In the context of dynamic
game theory, this principle is analogous to the concept of subgame perfect
equilibrium, although what constitutes an optimal policy in this case is
conditioned on the decision-maker's opponents choosing similarly optimal
policies from their points of view. subject to the constraints Here we are
choosing formula_22, knowing that our choice will cause the time 1 state to be
formula_23. That new state will then affect the decision problem from time 1
on. The whole future decision problem appears inside the square brackets on the
right. The Bellman equation. So far it seems we have only made the problem
uglier by separating today's decision from future decisions. But we can
simplify by noticing that what is inside the square brackets on the right is
"the value" of the time 1 decision problem, starting from state formula_23. The
Bellman equation is classified as a functional equation, because solving it
means finding the unknown function "V", which is the "value function". Recall
that the value function describes the best possible value of the objective, as
a function of the state "x". By calculating the value function, we will also
find the function "a"("x") that describes the optimal action as a function of
the state; this is called the "policy function". The Bellman equation in a
stochastic problem. Dynamic programming can be especially useful in stochastic
decisions, that is, optimization problems affected by random events. For
example, consider a problem exactly like the one discussed above, except that
formula_28 is a random variable, which may be influenced by formula_4 and
formula_7, but is not determined by them exactly. We can describe this case by
defining the probability distribution conditional on formula_4 and formula_7,
for example, Given this probability law determining formula_28 conditional on
formula_4 and formula_7, the Bellman equation can be written as where
formula_38 represents a conditional expectation under distribution "G".
Applications in economics. The first known application of a Bellman equation in
economics is due to Martin Beckmann and Richard Muth. Martin Beckmann also
wrote extensively on consumption theory using the Bellman equation in 1959. His
work influenced Edmund S. Phelps, among others. A celebrated economic
application of a Bellman equation is Merton's seminal 1973 article on the
intertemporal capital asset pricing model. (See also Merton's portfolio
problem).The solution to Merton's theoretical model, one in which investors
chose between income today and future income or capital gains, is a form of
Bellman's equation. Because economic applications of dynamic programming
usually result in a Bellman equation that is a difference equation, economists
refer to dynamic programming as a "recursive method." Stokey, Lucas & Prescott
describe stochastic and nonstochastic dynamic programming in considerable
detail, giving many examples of how to employ dynamic programming to solve
problems in economic theory. This book led to dynamic programming being
employed to solve a wide range of theoretical problems in economics, including
optimal economic growth, resource extraction, principalâ€“agent problems,
public finance, business investment, asset pricing, factor supply, and
industrial organization. Ljungqvist & Sargent apply dynamic programming to
study a variety of theoretical questions in monetary policy, fiscal policy,
taxation, economic growth, search theory, and labor economics. Dixit & Pindyck
showed the value of the method for thinking about capital budgeting. Anderson
adapted the technique to business valuation, including privately-held
businesses. Using dynamic programming to solve concrete problems is complicated
by informational difficulties, such as choosing the unobservable discount rate.
There are also computational issues, the main one being the curse of
dimensionality arising from the vast number of possible actions and potential
state variables that must be considered before an optimal strategy can be
selected. For an extensive discussion of computational issues, see Miranda &
Fackler., and Meyn 2007 Example. This equation describes the expected reward
for taking the action prescribed by some policy formula_39. It describes the
reward for taking the action giving the highest expected return.
